{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3eb479",
   "metadata": {},
   "source": [
    "To run the notebook on different machine, you need to adjust the following:\n",
    "- Original preprocessed data directory\n",
    "- Create 3 directories for ```nnUNet_raw```, ```nnUNet_preprocessed```, and ```nnUNet_results```\n",
    "- Correct directory for nnunet raw in  ```create_nnunet_dataset_structure()``` function\n",
    "- Correct directory in env variables\n",
    "- Pick up the correct env variable export cell for your platform (Linux or Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0e4b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822e05d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: nibabel in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: matplotlib in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: pandas in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: scipy in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: tqdm in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: plotly in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (6.3.0)\n",
      "Requirement already satisfied: optuna in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: SimpleITK in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (2.5.2)\n",
      "Requirement already satisfied: hiddenlayer in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (0.3)\n",
      "Requirement already satisfied: torch in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: packaging>=20 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from nibabel) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from nibabel) (4.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from plotly) (2.1.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from optuna) (1.16.4)\n",
      "Requirement already satisfied: colorlog in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: PyYAML in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: filelock in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: Mako in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: six>=1.5 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\rsa\\chimera-pcbr-main\\chimera_venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy nibabel matplotlib pandas scipy tqdm plotly optuna SimpleITK hiddenlayer torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c105ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581fc97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting causal-conv1d\n",
      "  Downloading causal_conv1d-1.5.2.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [26 lines of output]\n",
      "      C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-cwnhu_j_\\overlay\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.8.0+cpu\n",
      "      \n",
      "      \n",
      "      <string>:119: UserWarning: causal_conv1d was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "      Traceback (most recent call last):\n",
      "        File \"F:\\RSA\\chimera-pcbr-main\\chimera_venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"F:\\RSA\\chimera-pcbr-main\\chimera_venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"F:\\RSA\\chimera-pcbr-main\\chimera_venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-cwnhu_j_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-cwnhu_j_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-cwnhu_j_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 188, in <module>\n",
      "      NameError: name 'bare_metal_version' is not defined\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install causal-conv1d mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed43dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be done after installing pytorch\n",
    "# ! pip install nnunetv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/MrBlankness/LightM-UNet\n",
    "! cd LightM-UNet/lightm-unet\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.widgets import Slider\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "import optuna\n",
    "\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import hiddenlayer\n",
    "\n",
    "import mamba_ssm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475b1e7",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_dir = r\"F:\\aims_tbi\\normalized_T1_scans\"\n",
    "masks_dir = r\"F:\\aims_tbi\\resampled_1mm_Lesion_masks\"\n",
    "\n",
    "\n",
    "# print number of files in processed images and masks\n",
    "print(f\"Number of processed images: {len(os.listdir(images_dir))}\")\n",
    "print(f\"Number of processed masks: {len(os.listdir(masks_dir))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scan parameters\n",
    "scan_id = 'scan_0001'\n",
    "start_slice = 110\n",
    "num_slices = 5\n",
    "\n",
    "# Load the .nii.gz files\n",
    "image_path = os.path.join(images_dir, f\"{scan_id}_T1_normalized.nii.gz\")\n",
    "mask_path = os.path.join(masks_dir, f\"{scan_id}_Lesion_resampled_1mm.nii.gz\")\n",
    "\n",
    "# Load image and mask using nibabel\n",
    "image_nii = nib.load(image_path)\n",
    "mask_nii = nib.load(mask_path)\n",
    "\n",
    "image_array = image_nii.get_fdata().astype(np.float32)\n",
    "mask_array = mask_nii.get_fdata().astype(np.uint8)\n",
    "\n",
    "print(f\"Loaded {scan_id}:\")\n",
    "print(f\"Image shape: {image_array.shape}\")\n",
    "print(f\"Mask shape: {mask_array.shape}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, num_slices, figsize=(num_slices * 3, 10))\n",
    "\n",
    "# Ensure axes is 2D even for single slice\n",
    "if num_slices == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_slices):\n",
    "    slice_idx = start_slice + i\n",
    "\n",
    "    # Check if slice index is valid\n",
    "    if slice_idx >= image_array.shape[2]:\n",
    "        print(f\"⚠️ Slice {slice_idx} is out of bounds (max: {image_array.shape[2]-1}), skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract slices (transpose for proper orientation)\n",
    "    image_slice = image_array[:, :, slice_idx].T\n",
    "    mask_slice = mask_array[:, :, slice_idx].T\n",
    "\n",
    "    # Row 1: Processed T1 image only\n",
    "    axes[0, i].imshow(image_slice, cmap='gray', origin='lower')\n",
    "    axes[0, i].set_title(f'Processed T1\\n(Normalized) Slice {slice_idx}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Row 2: Processed T1 + Lesion overlay\n",
    "    axes[1, i].imshow(image_slice, cmap='gray', origin='lower')\n",
    "    if np.any(mask_slice > 0):  # Only overlay if there are lesions in this slice\n",
    "        axes[1, i].imshow(mask_slice, cmap='Reds', alpha=0.6, origin='lower')\n",
    "    axes[1, i].set_title(f'Processed T1 + Lesion\\nSlice {slice_idx}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Processed Images from NIfTI Files - {scan_id}', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print intensity statistics\n",
    "print(f\"\\n=== Statistics from NIfTI Files for {scan_id} ===\")\n",
    "\n",
    "# Image stats (brain voxels only)\n",
    "brain_mask = image_array != 0  # Background is 0 after normalization\n",
    "brain_voxels = image_array[brain_mask]\n",
    "\n",
    "print(\"Processed T1 Image (from NIfTI file):\")\n",
    "print(f\"  Mean: {np.mean(brain_voxels):.6f}\")\n",
    "print(f\"  Std: {np.std(brain_voxels):.6f}\")\n",
    "print(f\"  Min: {np.min(brain_voxels):.4f}\")\n",
    "print(f\"  Max: {np.max(brain_voxels):.4f}\")\n",
    "print(f\"  Shape: {image_array.shape}\")\n",
    "\n",
    "# Lesion statistics\n",
    "lesion_voxels = np.count_nonzero(mask_array)\n",
    "total_voxels = mask_array.size\n",
    "lesion_percentage = (lesion_voxels / total_voxels) * 100\n",
    "\n",
    "print(f\"\\nLesion Mask (from NIfTI file):\")\n",
    "print(f\"  Lesion voxels: {lesion_voxels:,}\")\n",
    "print(f\"  Total voxels: {total_voxels:,}\")\n",
    "print(f\"  Lesion percentage: {lesion_percentage:.4f}%\")\n",
    "print(f\"  Shape: {mask_array.shape}\")\n",
    "\n",
    "# If you have metadata, you can print it here (optional)\n",
    "# print(f\"\\nMetadata Statistics:\")\n",
    "# print(f\"  Brain voxels: ...\")\n",
    "# print(f\"  Brain mean: ...\")\n",
    "# print(f\"  Brain std: ...\")\n",
    "# print(f\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdc0a6",
   "metadata": {},
   "source": [
    "### Verify the mask values are limited to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd565b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fname in os.listdir(masks_dir):\n",
    "#     if fname.endswith(\"_Lesion_resampled_1mm.nii.gz\"):\n",
    "#         mask_path = os.path.join(masks_dir, fname)\n",
    "#         mask_array = nib.load(mask_path).get_fdata().astype(np.uint8)\n",
    "#         unique_vals = np.unique(mask_array)\n",
    "#         if np.any((unique_vals != 0) & (unique_vals != 1)):\n",
    "#             print(f\"{fname}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb538",
   "metadata": {},
   "source": [
    "## nnUNet setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d62bbb",
   "metadata": {},
   "source": [
    "### Dataset conversion for nnUNet compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a66e57",
   "metadata": {},
   "source": [
    "Create nnU-Net Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5073519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nnunet_dataset_structure():\n",
    "    \"\"\"Create nnU-Net compatible dataset structure\"\"\"\n",
    "    \n",
    "    # Set your nnUNet_raw path (adjust as needed)\n",
    "    nnunet_raw = \"F:\\\\aims_tbi\\\\nnUNet_raw\"  # or your path\n",
    "    dataset_name = \"Dataset600_TBILesion\"  # Choose an unused ID\n",
    "    \n",
    "    dataset_path = os.path.join(nnunet_raw, dataset_name)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(os.path.join(dataset_path, \"imagesTr\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_path, \"labelsTr\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_path, \"imagesTs\"), exist_ok=True)  # Optional for test data\n",
    "    \n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = create_nnunet_dataset_structure()\n",
    "print(f\"Created dataset structure at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c532c",
   "metadata": {},
   "source": [
    "Convert Your Files to nnU-Net Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ff199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nii_to_nnunet_format(images_dir, masks_dir, dataset_path, train_ratio=0.85):\n",
    "    \"\"\"\n",
    "    Convert NIfTI images and masks to nnU-Net format.\n",
    "    \"\"\"\n",
    "    # Get all scan IDs from image filenames\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith('_T1_normalized.nii.gz')]\n",
    "    scan_ids = [f.replace('_T1_normalized.nii.gz', '') for f in image_files]\n",
    "\n",
    "    print(f\"Found {len(scan_ids)} scans to convert\")\n",
    "\n",
    "    # Stratified split: lesion vs no-lesion (based on mask content)\n",
    "    lesion_scans = []\n",
    "    no_lesion_scans = []\n",
    "    for scan_id in scan_ids:\n",
    "        mask_path = os.path.join(masks_dir, f\"{scan_id}_Lesion_resampled_1mm.nii.gz\")\n",
    "        mask_array = nib.load(mask_path).get_fdata().astype(np.uint8)\n",
    "        if np.any(mask_array > 0):\n",
    "            lesion_scans.append(scan_id)\n",
    "        else:\n",
    "            no_lesion_scans.append(scan_id)\n",
    "\n",
    "    print(f\"Lesion scans: {len(lesion_scans)}\")\n",
    "    print(f\"No-lesion scans: {len(no_lesion_scans)}\")\n",
    "\n",
    "    n_train_lesion = int(len(lesion_scans) * train_ratio)\n",
    "    n_train_no_lesion = int(len(no_lesion_scans) * train_ratio)\n",
    "\n",
    "    train_ids = lesion_scans[:n_train_lesion] + no_lesion_scans[:n_train_no_lesion]\n",
    "    test_ids = lesion_scans[n_train_lesion:] + no_lesion_scans[n_train_no_lesion:]\n",
    "\n",
    "    print(f\"Training scans: {len(train_ids)}\")\n",
    "    print(f\"Test scans: {len(test_ids)}\")\n",
    "\n",
    "    # Create separate directory for test labels (for evaluation)\n",
    "    test_labels_dir = os.path.join(dataset_path, \"test_labels_for_evaluation\")\n",
    "    os.makedirs(test_labels_dir, exist_ok=True)\n",
    "\n",
    "    converted_count = 0\n",
    "\n",
    "    for split, ids in [(\"Tr\", train_ids), (\"Ts\", test_ids)]:\n",
    "        for scan_id in ids:\n",
    "            try:\n",
    "                image_path = os.path.join(images_dir, f\"{scan_id}_T1_normalized.nii.gz\")\n",
    "                mask_path = os.path.join(masks_dir, f\"{scan_id}_Lesion_resampled_1mm.nii.gz\")\n",
    "\n",
    "                image_nii = nib.load(image_path)\n",
    "                mask_nii = nib.load(mask_path)\n",
    "\n",
    "                image_array = image_nii.get_fdata().astype(np.float32)\n",
    "                mask_array = mask_nii.get_fdata().astype(np.uint8)\n",
    "                mask_array = (mask_array > 0).astype(np.uint8)  # binarize\n",
    "\n",
    "                affine = image_nii.affine\n",
    "\n",
    "                # Save images\n",
    "                image_filename = f\"{scan_id}_0000.nii.gz\"\n",
    "                image_save_path = os.path.join(dataset_path, f\"images{split}\", image_filename)\n",
    "                nib.save(nib.Nifti1Image(image_array, affine), image_save_path)\n",
    "\n",
    "                # Save masks\n",
    "                mask_filename = f\"{scan_id}.nii.gz\"\n",
    "                mask_nii_save = nib.Nifti1Image(mask_array, affine)\n",
    "                if split == \"Tr\":\n",
    "                    mask_save_path = os.path.join(dataset_path, \"labelsTr\", mask_filename)\n",
    "                else:\n",
    "                    mask_save_path = os.path.join(test_labels_dir, mask_filename)\n",
    "                nib.save(mask_nii_save, mask_save_path)\n",
    "\n",
    "                converted_count += 1\n",
    "                if converted_count % 50 == 0:\n",
    "                    print(f\"Converted {converted_count} scans...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {scan_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Successfully converted {converted_count} scans\")\n",
    "    print(f\"Test labels saved to: {test_labels_dir}\")\n",
    "\n",
    "    return len(train_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = convert_nii_to_nnunet_format(images_dir, masks_dir, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7365ef5",
   "metadata": {},
   "source": [
    "Create dataset.json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_json(dataset_path, num_training):\n",
    "    \"\"\"Create dataset.json file for nnU-Net\"\"\"\n",
    "    \n",
    "    dataset_json = {\n",
    "        \"channel_names\": {\n",
    "            \"0\": \"T1\"  # Your T1-weighted MRI scans\n",
    "        },\n",
    "        \"labels\": {\n",
    "            \"background\": 0,\n",
    "            \"lesion\": 1\n",
    "        },\n",
    "        \"numTraining\": num_training,\n",
    "        \"file_ending\": \".nii.gz\",\n",
    "        \"dataset_name\": \"TBI_Lesion_Segmentation\",\n",
    "        \"reference\": \"AIMS TBI Challenge\",\n",
    "        \"licence\": \"Your License\",\n",
    "        \"description\": \"Traumatic Brain Injury Lesion Segmentation Dataset\"\n",
    "    }\n",
    "    \n",
    "    # Save dataset.json\n",
    "    json_path = os.path.join(dataset_path, \"dataset.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_json, f, indent=2)\n",
    "    \n",
    "    print(f\"Created dataset.json with {num_training} training cases\")\n",
    "    print(f\"Saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_json(dataset_path, n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe8077",
   "metadata": {},
   "source": [
    "Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset_structure(dataset_path):\n",
    "    \"\"\"Verify the dataset structure is correct\"\"\"\n",
    "    \n",
    "    print(\"Verifying dataset structure...\")\n",
    "    \n",
    "    # Check folder structure\n",
    "    required_folders = [\"imagesTr\", \"labelsTr\"]\n",
    "    for folder in required_folders:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"❌ Missing folder: {folder}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"✅ Found folder: {folder}\")\n",
    "    \n",
    "    # Check dataset.json\n",
    "    json_path = os.path.join(dataset_path, \"dataset.json\")\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\"❌ Missing dataset.json\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Found dataset.json\")\n",
    "    \n",
    "    # Check file counts\n",
    "    images_tr = len([f for f in os.listdir(os.path.join(dataset_path, \"imagesTr\")) if f.endswith('.nii.gz')])\n",
    "    labels_tr = len([f for f in os.listdir(os.path.join(dataset_path, \"labelsTr\")) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    print(f\"Training images: {images_tr}\")\n",
    "    print(f\"Training labels: {labels_tr}\")\n",
    "    \n",
    "    if images_tr != labels_tr:\n",
    "        print(\"❌ Mismatch between number of images and labels\")\n",
    "        return False\n",
    "    \n",
    "    # Check file naming convention\n",
    "    sample_files = os.listdir(os.path.join(dataset_path, \"imagesTr\"))[:5]\n",
    "    for file in sample_files:\n",
    "        if not file.endswith('_0000.nii.gz'):\n",
    "            print(f\"❌ Incorrect naming: {file} (should end with _0000.nii.gz)\")\n",
    "            return False\n",
    "    \n",
    "    print(\"✅ Dataset structure verification passed!\")\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdda8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_dataset_structure(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5ef7b",
   "metadata": {},
   "source": [
    "### env variable setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619258d",
   "metadata": {},
   "source": [
    "For Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! export nnUNet_raw=\"/media/fabian/nnUNet_raw\"\n",
    "# ! export nnUNet_preprocessed=\"/media/fabian/nnUNet_preprocessed\"\n",
    "# ! export nnUNet_results=\"/media/fabian/nnUNet_results\"\n",
    "# ! export nnUNet_n_proc_DA=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc0b99",
   "metadata": {},
   "source": [
    "For windows (PowerShell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['nnUNet_raw'] = \"F:\\\\aims_tbi\\\\nnUNet_raw\"\n",
    "os.environ['nnUNet_preprocessed'] = \"F:\\\\aims_tbi\\\\nnUNet_preprocessed\"\n",
    "os.environ['nnUNet_results'] = \"F:\\\\aims_tbi\\\\nnUNet_results\"\n",
    "os.environ['nnUNet_n_proc_DA'] = \"12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7de1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the environment variables are set\n",
    "print(\"Environment variables set:\")\n",
    "print(f\"nnUNet_raw: {os.environ.get('nnUNet_raw')}\")\n",
    "print(f\"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}\")\n",
    "print(f\"nnUNet_results: {os.environ.get('nnUNet_results')}\")\n",
    "print(f\"nnUNet_n_proc_DA: {os.environ.get('nnUNet_n_proc_DA')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21d990",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nnUNetv2_plan_and_preprocess -d 600 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for fold 0\n",
    "! nnUNetv2_train 600 3d_fullres all -tr nnUNetTrainerLightMUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chimera_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
