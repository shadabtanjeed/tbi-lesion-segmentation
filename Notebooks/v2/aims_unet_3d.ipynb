{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5a0e4b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy nibabel matplotlib pandas scipy tqdm plotly optuna SimpleITK hiddenlayer torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install causal-conv1d mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed43dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be done after installing pytorch\n",
    "# ! pip install nnunetv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/MrBlankness/LightM-UNet\n",
    "# ! cd LightM-UNet/lightm-unet\n",
    "# ! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.ndimage import zoom, rotate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.widgets import Slider\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import SimpleITK as sitk\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475b1e7",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_dir = r\"F:\\aims_tbi\\normalized_T1_scans\"\n",
    "masks_dir = r\"F:\\aims_tbi\\resampled_1mm_Lesion_masks\"\n",
    "\n",
    "\n",
    "# print number of files in processed images and masks\n",
    "print(f\"Number of processed images: {len(os.listdir(images_dir))}\")\n",
    "print(f\"Number of processed masks: {len(os.listdir(masks_dir))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scan parameters\n",
    "scan_id = 'scan_0001'\n",
    "start_slice = 110\n",
    "num_slices = 5\n",
    "\n",
    "# Load the .nii.gz files\n",
    "image_path = os.path.join(images_dir, f\"{scan_id}_T1_normalized.nii.gz\")\n",
    "mask_path = os.path.join(masks_dir, f\"{scan_id}_Lesion_resampled_1mm.nii.gz\")\n",
    "\n",
    "# Load image and mask using nibabel\n",
    "image_nii = nib.load(image_path)\n",
    "mask_nii = nib.load(mask_path)\n",
    "\n",
    "image_array = image_nii.get_fdata().astype(np.float32)\n",
    "mask_array = mask_nii.get_fdata().astype(np.uint8)\n",
    "\n",
    "print(f\"Loaded {scan_id}:\")\n",
    "print(f\"Image shape: {image_array.shape}\")\n",
    "print(f\"Mask shape: {mask_array.shape}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, num_slices, figsize=(num_slices * 3, 10))\n",
    "\n",
    "# Ensure axes is 2D even for single slice\n",
    "if num_slices == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_slices):\n",
    "    slice_idx = start_slice + i\n",
    "\n",
    "    # Check if slice index is valid\n",
    "    if slice_idx >= image_array.shape[2]:\n",
    "        print(f\"⚠️ Slice {slice_idx} is out of bounds (max: {image_array.shape[2]-1}), skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract slices (transpose for proper orientation)\n",
    "    image_slice = image_array[:, :, slice_idx].T\n",
    "    mask_slice = mask_array[:, :, slice_idx].T\n",
    "\n",
    "    # Row 1: Processed T1 image only\n",
    "    axes[0, i].imshow(image_slice, cmap='gray', origin='lower')\n",
    "    axes[0, i].set_title(f'Processed T1\\n(Normalized) Slice {slice_idx}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Row 2: Processed T1 + Lesion overlay\n",
    "    axes[1, i].imshow(image_slice, cmap='gray', origin='lower')\n",
    "    if np.any(mask_slice > 0):  # Only overlay if there are lesions in this slice\n",
    "        axes[1, i].imshow(mask_slice, cmap='Reds', alpha=0.6, origin='lower')\n",
    "    axes[1, i].set_title(f'Processed T1 + Lesion\\nSlice {slice_idx}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Processed Images from NIfTI Files - {scan_id}', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print intensity statistics\n",
    "print(f\"\\n=== Statistics from NIfTI Files for {scan_id} ===\")\n",
    "\n",
    "# Image stats (brain voxels only)\n",
    "brain_mask = image_array != 0  # Background is 0 after normalization\n",
    "brain_voxels = image_array[brain_mask]\n",
    "\n",
    "print(\"Processed T1 Image (from NIfTI file):\")\n",
    "print(f\"  Mean: {np.mean(brain_voxels):.6f}\")\n",
    "print(f\"  Std: {np.std(brain_voxels):.6f}\")\n",
    "print(f\"  Min: {np.min(brain_voxels):.4f}\")\n",
    "print(f\"  Max: {np.max(brain_voxels):.4f}\")\n",
    "print(f\"  Shape: {image_array.shape}\")\n",
    "\n",
    "# Lesion statistics\n",
    "lesion_voxels = np.count_nonzero(mask_array)\n",
    "total_voxels = mask_array.size\n",
    "lesion_percentage = (lesion_voxels / total_voxels) * 100\n",
    "\n",
    "print(f\"\\nLesion Mask (from NIfTI file):\")\n",
    "print(f\"  Lesion voxels: {lesion_voxels:,}\")\n",
    "print(f\"  Total voxels: {total_voxels:,}\")\n",
    "print(f\"  Lesion percentage: {lesion_percentage:.4f}%\")\n",
    "print(f\"  Shape: {mask_array.shape}\")\n",
    "\n",
    "# If you have metadata, you can print it here (optional)\n",
    "# print(f\"\\nMetadata Statistics:\")\n",
    "# print(f\"  Brain voxels: ...\")\n",
    "# print(f\"  Brain mean: ...\")\n",
    "# print(f\"  Brain std: ...\")\n",
    "# print(f\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdc0a6",
   "metadata": {},
   "source": [
    "### Verify the mask values are limited to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd565b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fname in os.listdir(masks_dir):\n",
    "#     if fname.endswith(\"_Lesion_resampled_1mm.nii.gz\"):\n",
    "#         mask_path = os.path.join(masks_dir, fname)\n",
    "#         mask_array = nib.load(mask_path).get_fdata().astype(np.uint8)\n",
    "#         unique_vals = np.unique(mask_array)\n",
    "#         if np.any((unique_vals != 0) & (unique_vals != 1)):\n",
    "#             print(f\"{fname}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb538",
   "metadata": {},
   "source": [
    "## 3D-UNet setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af8033",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LesionDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, patch_size=(64, 64, 64), transform=None):\n",
    "        self.images = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii.gz')])\n",
    "        self.masks = sorted([f for f in os.listdir(masks_dir) if f.endswith('.nii.gz')])\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        image = nib.load(img_path).get_fdata().astype(np.float32)\n",
    "        mask = nib.load(mask_path).get_fdata().astype(np.float32)\n",
    "        # Add channel dimension\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        # Random crop\n",
    "        D, H, W = image.shape[1:]\n",
    "        pd, ph, pw = self.patch_size\n",
    "        if D > pd and H > ph and W > pw:\n",
    "            d = np.random.randint(0, D - pd + 1)\n",
    "            h = np.random.randint(0, H - ph + 1)\n",
    "            w = np.random.randint(0, W - pw + 1)\n",
    "            image = image[:, d:d+pd, h:h+ph, w:w+pw]\n",
    "            mask = mask[:, d:d+pd, h:h+ph, w:w+pw]\n",
    "        return torch.from_numpy(image), torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c875a6",
   "metadata": {},
   "source": [
    "### 3D U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
    "        super(UNet3D, self).__init__()\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.encoder2 = self._block(features, features*2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.encoder3 = self._block(features*2, features*4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        self.encoder4 = self._block(features*4, features*8)\n",
    "        self.pool4 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.bottleneck = self._block(features*8, features*16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose3d(features*16, features*8, 2, stride=2)\n",
    "        self.decoder4 = self._block(features*16, features*8)\n",
    "        self.up3 = nn.ConvTranspose3d(features*8, features*4, 2, stride=2)\n",
    "        self.decoder3 = self._block(features*8, features*4)\n",
    "        self.up2 = nn.ConvTranspose3d(features*4, features*2, 2, stride=2)\n",
    "        self.decoder2 = self._block(features*4, features*2)\n",
    "        self.up1 = nn.ConvTranspose3d(features*2, features, 2, stride=2)\n",
    "        self.decoder1 = self._block(features*2, features)\n",
    "\n",
    "        self.conv = nn.Conv3d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def _block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.up4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.up3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.up2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.up1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063dd85",
   "metadata": {},
   "source": [
    "### Dice Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(pred, target, epsilon=1e-6):\n",
    "    pred = (pred > 0.5).float()\n",
    "    target = target.float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "    return dice.item()\n",
    "\n",
    "def dice_per_class(pred, target, epsilon=1e-6):\n",
    "    pred = (pred > 0.5).float()\n",
    "    target = target.float()\n",
    "    # Lesion (positive)\n",
    "    dice_lesion = dice_score(pred, target, epsilon)\n",
    "    # Background (negative)\n",
    "    dice_bg = dice_score(1 - pred, 1 - target, epsilon)\n",
    "    # Overall (mean)\n",
    "    dice_overall = (dice_lesion + dice_bg) / 2\n",
    "    return dice_lesion, dice_bg, dice_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac4588",
   "metadata": {},
   "source": [
    "### Loss Function (Dice + BCE for Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cffefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        dice = 1 - dice_score(pred, target)\n",
    "        bce = self.bce(pred, target)\n",
    "        return dice + bce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccca360",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and DataLoader \n",
    "dataset = LesionDataset(images_dir, masks_dir, patch_size=(64, 64, 64))\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = UNet3D(init_features=16).to(device)\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "num_epochs = 50\n",
    "best_val_dice = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    dice_lesion, dice_bg, dice_overall = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            d_lesion, d_bg, d_overall = dice_per_class(outputs, masks)\n",
    "            dice_lesion += d_lesion\n",
    "            dice_bg += d_bg\n",
    "            dice_overall += d_overall\n",
    "    val_loss /= len(val_loader)\n",
    "    dice_lesion /= len(val_loader)\n",
    "    dice_bg /= len(val_loader)\n",
    "    dice_overall /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Dice Lesion={dice_lesion:.4f}, Dice No Lesion={dice_bg:.4f}, Dice Overall={dice_overall:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if dice_lesion > best_val_dice:\n",
    "        best_val_dice = dice_lesion\n",
    "        torch.save(model.state_dict(), \"3dunet_model_new_preprocessing.pth\")\n",
    "        print(\"Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397121e3",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model for inference\n",
    "# model = UNet3D().to(device)\n",
    "# model.load_state_dict(torch.load(\"best_3dunet_model.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# # Example inference on a single scan\n",
    "# with torch.no_grad():\n",
    "#     img, _ = dataset[0]\n",
    "#     img = img.unsqueeze(0).to(device)\n",
    "#     pred = model(img)\n",
    "#     pred_mask = (pred > 0.5).cpu().numpy().astype(np.uint8)[0,0]\n",
    "#     # Save as NIfTI\n",
    "#     nib.save(nib.Nifti1Image(pred_mask, np.eye(4)), \"predicted_mask.nii.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chimera_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
